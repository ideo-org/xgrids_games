{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install and Import modules\n",
    "Install [PyTorch3D](https://github.com/facebookresearch/pytorch3d) and import modules we need."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!conda install pytorch3d -c pytorch3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install matplotlib pytorch3d\n",
    "!pip install scikit-image matplotlib imageio plotly opencv-python\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
    "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
    "from pytorch3d.renderer import (\n",
    "    PerspectiveCameras,\n",
    "    AmbientLights,\n",
    "    Materials,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    SoftPhongShader,\n",
    ")\n",
    "\n",
    "# Add path for demo utils functions\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(''))\n",
    "\n",
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load a mesh and texture file\n",
    "Load an `.obj` file and its associated `.mtl` file and create a Textures and Meshes object. We use [**spot**](http://www.cs.cmu.edu/~kmcrane/Projects/ModelRepository/index.html#spot) as an example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set paths\n",
    "DATA_DIR = \"./data_sample\"\n",
    "obj_filename = os.path.join(DATA_DIR, \"spot/spot.obj\")\n",
    "\n",
    "# Load obj file\n",
    "mesh = load_objs_as_meshes([obj_filename], device=device)\n",
    "\n",
    "# Visualize the texture map\n",
    "plt.figure(figsize=(7,7))\n",
    "texture_image=mesh.textures.maps_padded()\n",
    "plt.imshow(texture_image.squeeze().cpu().numpy())\n",
    "plt.axis(\"off\");"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load SFM data\n",
    "The following module shows how to load structure-from-motion (SFM) data provided by us. The original spot dataset does not include pose information so we load poses from other scene to just illustrate this method. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "import math\n",
    "import imageio.v2 as imageio\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def qvec2rotmat(qvec):\n",
    "    return np.array([\n",
    "        [\n",
    "                    1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n",
    "                    2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n",
    "                    2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]\n",
    "                    ], [\n",
    "            2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n",
    "            1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n",
    "            2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]\n",
    "        ], [\n",
    "            2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n",
    "            2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n",
    "            1 - 2 * qvec[1]**2 - 2 * qvec[2]**2\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "\n",
    "def load_sfm_data(basedir, testskip=1):\n",
    "    with open(os.path.join(basedir, \"colmap/cameras.txt\"), \"r\") as f:\n",
    "        angle_x = math.pi / 2\n",
    "        for line in f:\n",
    "            # 1 SIMPLE_RADIAL 2048 1536 1580.46 1024 768 0.0045691\n",
    "            # 1 OPENCV 3840 2160 3178.27 3182.09 1920 1080 0.159668 -0.231286 -0.00123982 0.00272224\n",
    "            # 1 RADIAL 1920 1080 1665.1 960 540 0.0672856 -0.0761443\n",
    "            if line[0] == \"#\":\n",
    "                continue\n",
    "            els = line.split(\" \")\n",
    "            w = float(els[2])\n",
    "            h = float(els[3])\n",
    "            image_size = (int(h), int(w))\n",
    "            fl_x = float(els[4])\n",
    "            fl_y = float(els[4])\n",
    "            k1 = 0\n",
    "            k2 = 0\n",
    "            p1 = 0\n",
    "            p2 = 0\n",
    "            cx = w / 2\n",
    "            cy = h / 2\n",
    "            if els[1] == \"SIMPLE_PINHOLE\":\n",
    "                cx = float(els[5])\n",
    "                cy = float(els[6])\n",
    "            elif els[1] == \"PINHOLE\":\n",
    "                fl_y = float(els[5])\n",
    "                cx = float(els[6])\n",
    "                cy = float(els[7])\n",
    "            elif els[1] == \"SIMPLE_RADIAL\":\n",
    "                cx = float(els[5])\n",
    "                cy = float(els[6])\n",
    "                k1 = float(els[7])\n",
    "            elif els[1] == \"RADIAL\":\n",
    "                cx = float(els[5])\n",
    "                cy = float(els[6])\n",
    "                k1 = float(els[7])\n",
    "                k2 = float(els[8])\n",
    "            elif els[1] == \"OPENCV\":\n",
    "                fl_y = float(els[5])\n",
    "                cx = float(els[6])\n",
    "                cy = float(els[7])\n",
    "                k1 = float(els[8])\n",
    "                k2 = float(els[9])\n",
    "                p1 = float(els[10])\n",
    "                p2 = float(els[11])\n",
    "            else:\n",
    "                print(\"unknown camera model \", els[1])\n",
    "            angle_x = math.atan(w / (fl_x * 2)) * 2\n",
    "            angle_y = math.atan(h / (fl_y * 2)) * 2\n",
    "            fovx = angle_x * 180 / math.pi\n",
    "            fovy = angle_y * 180 / math.pi\n",
    "\n",
    "    with open(os.path.join(basedir, \"colmap/images.txt\"), \"r\") as f:\n",
    "        i = 0\n",
    "        metas = []\n",
    "        bottom = np.array([0.0, 0.0, 0.0, 1.0]).reshape([1, 4])\n",
    "        up = np.zeros(3)\n",
    "        image_folder = os.path.join(basedir, \"images/\")\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line[0] == \"#\":\n",
    "                continue\n",
    "            i = i + 1\n",
    "            if i % 2 == 1:\n",
    "                # 1-4 is quat, 5-7 is trans, 9ff is filename (9, if filename contains no spaces)\n",
    "                elems = line.split(\" \")\n",
    "                name = '_'.join(elems[9:])\n",
    "                image_id = int(elems[0])\n",
    "                qvec = np.array(tuple(map(float, elems[1:5])))\n",
    "                tvec = np.array(tuple(map(float, elems[5:8])))\n",
    "                R = qvec2rotmat(-qvec)\n",
    "                t = tvec.reshape([3, 1])\n",
    "                m = np.concatenate([np.concatenate([R, t], 1), bottom], 0)\n",
    "                meta = {'pose': m, 'name': name}\n",
    "                metas.append(meta)\n",
    "\n",
    "    return metas, fl_x, fl_y, cx, cy, image_size\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Coordinate transformation\n",
    "PyTorch3D uses left-up-front (+X left, +Y up, +Z in) system while colmap adopts right-down-front (+X right, +Y down, +Z in, just like OpenCV) format. Therefore, we need to convert local camera coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scene_dir = './data_sample/spot/'\n",
    "metas, fx, fy, cx, cy, image_size = load_sfm_data(scene_dir, testskip=1)\n",
    "\n",
    "torch3d_T_colmap = np.array([[-1, 0, 0], [0, -1, 0], [0, 0, 1]])\n",
    "\n",
    "for meta in metas:\n",
    "    pose = meta['pose']\n",
    "    R = pose[0:3, 0:3]\n",
    "    T = pose[0:3, -1]\n",
    "    R = (torch3d_T_colmap @ R).T\n",
    "    T = (torch3d_T_colmap @ T)\n",
    "    R = torch.from_numpy(R).unsqueeze(0)\n",
    "    T = torch.from_numpy(T).unsqueeze(0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create a renderer\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize a camera.\n",
    "# With world coordinates +Y up, +X left and +Z in, the front of the cow is facing the -Z direction.\n",
    "cameras = PerspectiveCameras(device=device, R=R, T=T, focal_length=(\n",
    "    (fx, fy),), principal_point=((cx, cy),), in_ndc=False, image_size=(image_size,))\n",
    "print(image_size)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# (h,w). As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
    "# and blur_radius=0.0. We also set bin_size and max_faces_per_bin to None which ensure that\n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for\n",
    "# explanations of these parameters. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=image_size,\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    ")\n",
    "\n",
    "# Place a ambient light to the object.\n",
    "lights = AmbientLights(device=device, ambient_color=None)\n",
    "\n",
    "# Create a Phong renderer by composing a rasterizer and a shader. The textured Phong shader will\n",
    "# interpolate the texture uv coordinates for each vertex, sample from a texture image and\n",
    "# apply the Phong lighting model\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=cameras,\n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=SoftPhongShader(\n",
    "        device=device,\n",
    "        cameras=cameras,\n",
    "        lights=lights\n",
    "    )\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Render the mesh\n",
    "Render the `spot.obj` with given pose."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# render\n",
    "images = renderer(mesh)\n",
    "\n",
    "# display\n",
    "plt.figure(figsize=(10.08, 7.56), dpi=100, frameon=False)\n",
    "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "plt.axis(\"off\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98ab565718e55ac02e12b8979586bd9c3ab46b75b36daa81234d05e47e147eda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
